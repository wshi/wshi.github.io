<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Why Formatting Matters? | Wei's Blog</title>
<meta name=keywords content="tech,AI"><meta name=description content="Introduction: What’s the Deal with Precision?
Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts.
I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond."><meta name=author content><link rel=canonical href=https://wshi.github.io/posts/why-formatting-matters/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://wshi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wshi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wshi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://wshi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://wshi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wshi.github.io/posts/why-formatting-matters/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://wshi.github.io/posts/why-formatting-matters/"><meta property="og:site_name" content="Wei's Blog"><meta property="og:title" content="Why Formatting Matters?"><meta property="og:description" content="Introduction: What’s the Deal with Precision? Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts. I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-22T20:00:00+08:00"><meta property="article:modified_time" content="2025-02-22T20:00:00+08:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Why Formatting Matters?"><meta name=twitter:description content="Introduction: What’s the Deal with Precision?
Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts.
I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wshi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Why Formatting Matters?","item":"https://wshi.github.io/posts/why-formatting-matters/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Why Formatting Matters?","name":"Why Formatting Matters?","description":"Introduction: What’s the Deal with Precision? Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts. I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond.\n","keywords":["tech","AI"],"articleBody":"Introduction: What’s the Deal with Precision? Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts. I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond.\nFloating-Point Formats: The Essentials What are these formats anyway? Floating-point numbers use an exponent and mantissa to store real values. Here’s the lineup:\nFP32: 32 bits (8 exponent, 23 mantissa). High precision, big memory footprint. BF16: 16 bits (8 exponent, 7 mantissa). Matches FP32’s range, halves the size. FP8: 8 bits (e.g., E4M3: 4 exponent, 3 mantissa). Tiny but temperamental. Format Bits Exponent Mantissa Range Memory FP32 32 8 23 ~10-38 to 1038 4B BF16 16 8 7 ~10-38 to 1038 2B FP8 8 4/5 3/2 ~10-7 to 108 1B Why cares? These trade-offs—precision vs. efficiency—drive AI’s biggest decisions.\n","wordCount":"196","inLanguage":"en","datePublished":"2025-02-22T20:00:00+08:00","dateModified":"2025-02-22T20:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://wshi.github.io/posts/why-formatting-matters/"},"publisher":{"@type":"Organization","name":"Wei's Blog","logo":{"@type":"ImageObject","url":"https://wshi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wshi.github.io/ accesskey=h title="Wei's Blog (Alt + H)">Wei's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wshi.github.io/ title=Home><span>Home</span></a></li><li><a href=https://wshi.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://wshi.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://wshi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://wshi.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Why Formatting Matters?
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2025-02-22 20:00:00 +0800 +0800'>February 22, 2025</span>&nbsp;·&nbsp;1 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction-whats-the-deal-with-precision aria-label="Introduction: What’s the Deal with Precision?">Introduction: What’s the Deal with Precision?</a></li><li><a href=#floating-point-formats-the-essentials aria-label="Floating-Point Formats: The Essentials">Floating-Point Formats: The Essentials</a></li></ul></div></details></div><div class=post-content><h1 id=introduction-whats-the-deal-with-precision>Introduction: What’s the Deal with Precision?<a hidden class=anchor aria-hidden=true href=#introduction-whats-the-deal-with-precision>#</a></h1><p>Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts.
I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond.</p><h1 id=floating-point-formats-the-essentials>Floating-Point Formats: The Essentials<a hidden class=anchor aria-hidden=true href=#floating-point-formats-the-essentials>#</a></h1><p>What are these formats anyway? Floating-point numbers use an exponent and mantissa to store real values. Here’s the lineup:</p><ul><li>FP32: 32 bits (8 exponent, 23 mantissa). High precision, big memory footprint.</li><li>BF16: 16 bits (8 exponent, 7 mantissa). Matches FP32’s range, halves the size.</li><li>FP8: 8 bits (e.g., E4M3: 4 exponent, 3 mantissa). Tiny but temperamental.</li></ul><table><thead><tr><th style=text-align:center>Format</th><th style=text-align:center>Bits</th><th style=text-align:center>Exponent</th><th style=text-align:center>Mantissa</th><th style=text-align:center>Range</th><th style=text-align:center>Memory</th></tr></thead><tbody><tr><td style=text-align:center>FP32</td><td style=text-align:center>32</td><td style=text-align:center>8</td><td style=text-align:center>23</td><td style=text-align:center>~10-38 to 1038</td><td style=text-align:center>4B</td></tr><tr><td style=text-align:center>BF16</td><td style=text-align:center>16</td><td style=text-align:center>8</td><td style=text-align:center>7</td><td style=text-align:center>~10-38 to 1038</td><td style=text-align:center>2B</td></tr><tr><td style=text-align:center>FP8</td><td style=text-align:center>8</td><td style=text-align:center>4/5</td><td style=text-align:center>3/2</td><td style=text-align:center>~10-7 to 108</td><td style=text-align:center>1B</td></tr></tbody></table><p>Why cares? These trade-offs—precision vs. efficiency—drive AI’s biggest decisions.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://wshi.github.io/tags/tech/>Tech</a></li><li><a href=https://wshi.github.io/tags/ai/>AI</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://wshi.github.io/>Wei's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>