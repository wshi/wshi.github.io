<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI on Wei's Blog</title><link>https://wshi.github.io/tags/ai/</link><description>Recent content in AI on Wei's Blog</description><generator>Hugo -- 0.144.2</generator><language>en-us</language><lastBuildDate>Sat, 22 Feb 2025 20:00:00 +0800</lastBuildDate><atom:link href="https://wshi.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Formatting Matters?</title><link>https://wshi.github.io/posts/why-formatting-matters/</link><pubDate>Sat, 22 Feb 2025 20:00:00 +0800</pubDate><guid>https://wshi.github.io/posts/why-formatting-matters/</guid><description>&lt;h1 id="introduction-whats-the-deal-with-precision">Introduction: What’s the Deal with Precision?&lt;/h1>
&lt;p>Why does switching from FP32 to FP8 turn a coherent chatbot into a babbling mess? Floating-point precision isn’t just a nerdy detail—it’s the heartbeat of large-scale AI. In a world of trillion-parameter models, formats like FP32, BF16, and FP8 decide if training converges, inference flies, or your GPU melts.
I’ve been digging into this lately, especially with models like DeepSeek V3 pushing low-precision boundaries. Here’s what I found: precision shapes everything from compute costs to output quality. Let’s break it down—starting with the basics, then diving into training, inference, and beyond.&lt;/p></description></item></channel></rss>